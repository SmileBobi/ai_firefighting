"""
åŸºäºVision Transformer (ViT) çš„å›¾åƒåˆ†ç±»ç¨‹åº
ä½¿ç”¨HuggingFace Transformersåº“å®ç°å›¾åƒåˆ†ç±»ä»»åŠ¡
æ”¯æŒå¤šç§æ•°æ®é›†å’Œæ¨¡å‹å¾®è°ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import os
import time
import json
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥HuggingFaceåº“
try:
    from transformers import (
        ViTForImageClassification, ViTImageProcessor, ViTConfig,
        TrainingArguments, Trainer, AutoImageProcessor, AutoModelForImageClassification
    )
    from datasets import load_dataset
    import PIL.Image
    HF_AVAILABLE = True
    print("âœ… HuggingFace Transformersåº“å¯ç”¨")
except ImportError:
    HF_AVAILABLE = False
    print("âŒ HuggingFace Transformersåº“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

class ImageDataset(Dataset):
    """
    å›¾åƒæ•°æ®é›†å¤„ç†ç±»
    å¤„ç†å›¾åƒæ•°æ®ï¼Œæ”¯æŒè®­ç»ƒå’ŒéªŒè¯
    """
    def __init__(self, images, labels, processor, transform=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        Args:
            images: å›¾åƒåˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            processor: å›¾åƒå¤„ç†å™¨
            transform: æ•°æ®å¢å¼ºå˜æ¢
        """
        self.images = images
        self.labels = labels
        self.processor = processor
        self.transform = transform
        
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.images)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        Args:
            idx: æ ·æœ¬ç´¢å¼•
        Returns:
            å¤„ç†åçš„å›¾åƒå’Œæ ‡ç­¾
        """
        image = self.images[idx]
        label = self.labels[idx]
        
        # å¦‚æœæ˜¯PILå›¾åƒï¼Œç›´æ¥ä½¿ç”¨
        if hasattr(image, 'convert'):
            if image.mode != 'RGB':
                image = image.convert('RGB')
        else:
            # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºPILå›¾åƒ
            if isinstance(image, np.ndarray):
                if image.dtype != np.uint8:
                    image = (image * 255).astype(np.uint8)
                image = PIL.Image.fromarray(image)
            else:
                # åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒ
                image = PIL.Image.new('RGB', (224, 224), color=(128, 128, 128))
        
        # åº”ç”¨æ•°æ®å¢å¼º
        if self.transform:
            image = self.transform(image)
        
        # ä½¿ç”¨å¤„ç†å™¨å¤„ç†å›¾åƒ
        if HF_AVAILABLE:
            inputs = self.processor(images=image, return_tensors="pt")
            return {
                'pixel_values': inputs['pixel_values'].squeeze(0),
                'labels': torch.tensor(label, dtype=torch.long)
            }
        else:
            # æ¨¡æ‹Ÿå¤„ç†
            return {
                'pixel_values': torch.randn(3, 224, 224),
                'labels': torch.tensor(label, dtype=torch.long)
            }

class ViTImageClassifier:
    """
    Vision Transformerå›¾åƒåˆ†ç±»å™¨
    åŸºäºViTæ¨¡å‹å®ç°å›¾åƒåˆ†ç±»åŠŸèƒ½
    """
    def __init__(self, model_name='google/vit-base-patch16-224', num_labels=10, device=None):
        """
        åˆå§‹åŒ–å›¾åƒåˆ†ç±»å™¨
        Args:
            model_name: æ¨¡å‹åç§°
            num_labels: åˆ†ç±»ç±»åˆ«æ•°
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_name = model_name
        self.num_labels = num_labels
        
        if HF_AVAILABLE:
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå¤„ç†å™¨
            self.processor = ViTImageProcessor.from_pretrained(model_name)
            self.model = ViTForImageClassification.from_pretrained(
                model_name,
                num_labels=num_labels,
                ignore_mismatched_sizes=True
            )
            self.model.to(self.device)
            print(f"âœ… åŠ è½½ViTæ¨¡å‹: {model_name}")
        else:
            # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
            self.processor = self._create_mock_processor()
            self.model = self._create_mock_model()
            print("âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ï¼ˆHuggingFaceåº“ä¸å¯ç”¨ï¼‰")
    
    def _create_mock_processor(self):
        """åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒå¤„ç†å™¨"""
        class MockProcessor:
            def __call__(self, images, return_tensors="pt", **kwargs):
                return {
                    'pixel_values': torch.randn(1, 3, 224, 224)
                }
        
        return MockProcessor()
    
    def _create_mock_model(self):
        """åˆ›å»ºæ¨¡æ‹ŸViTæ¨¡å‹"""
        class MockModel:
            def __init__(self):
                self.config = type('Config', (), {'num_labels': self.num_labels})()
            
            def forward(self, pixel_values, labels=None):
                """æ¨¡æ‹Ÿå‰å‘ä¼ æ’­"""
                batch_size = pixel_values.size(0)
                logits = torch.randn(batch_size, self.num_labels)
                
                if labels is not None:
                    loss = nn.CrossEntropyLoss()(logits, labels)
                    return type('Output', (), {'loss': loss, 'logits': logits})()
                else:
                    return type('Output', (), {'logits': logits})()
            
            def to(self, device):
                return self
            
            def train(self):
                pass
            
            def eval(self):
                pass
        
        return MockModel()
    
    def predict(self, images):
        """
        é¢„æµ‹å›¾åƒç±»åˆ«
        Args:
            images: è¾“å…¥å›¾åƒåˆ—è¡¨
        Returns:
            é¢„æµ‹ç»“æœ
        """
        self.model.eval()
        predictions = []
        probabilities = []
        
        with torch.no_grad():
            for image in images:
                if HF_AVAILABLE:
                    # å¤„ç†å•ä¸ªå›¾åƒ
                    inputs = self.processor(images=image, return_tensors="pt").to(self.device)
                    outputs = self.model(**inputs)
                    logits = outputs.logits
                else:
                    # æ¨¡æ‹Ÿé¢„æµ‹
                    logits = torch.randn(1, self.num_labels)
                
                # è®¡ç®—æ¦‚ç‡
                probs = torch.softmax(logits, dim=-1)
                pred = torch.argmax(probs, dim=-1)
                
                predictions.append(pred.cpu().numpy()[0])
                probabilities.append(probs.cpu().numpy()[0])
        
        return predictions, probabilities

class ImageDataProcessor:
    """
    å›¾åƒæ•°æ®å¤„ç†å™¨
    å¤„ç†å›¾åƒæ•°æ®ï¼Œå‡†å¤‡è®­ç»ƒæ•°æ®
    """
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨"""
        self.images = []
        self.labels = []
        self.class_names = []
    
    def load_cifar10_data(self):
        """
        åŠ è½½CIFAR-10æ•°æ®é›†
        Returns:
            å›¾åƒå’Œæ ‡ç­¾åˆ—è¡¨
        """
        if HF_AVAILABLE:
            try:
                # ä½¿ç”¨HuggingFace datasetsåº“åŠ è½½CIFAR-10
                dataset = load_dataset('cifar10', split='train')
                images = [PIL.Image.fromarray(np.array(item['img'])) for item in dataset]
                labels = [item['label'] for item in dataset]
                class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
                              'dog', 'frog', 'horse', 'ship', 'truck']
                print(f"âœ… åŠ è½½CIFAR-10æ•°æ®: {len(images)}å¼ å›¾åƒ")
                return images, labels, class_names
            except Exception as e:
                print(f"âŒ åŠ è½½CIFAR-10å¤±è´¥: {e}")
                return self._create_mock_data()
        else:
            return self._create_mock_data()
    
    def _create_mock_data(self):
        """åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒæ•°æ®"""
        print("âš ï¸ åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒæ•°æ®")
        
        # åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒ
        images = []
        labels = []
        class_names = ['cat', 'dog', 'bird', 'car', 'tree', 'house', 'flower', 'mountain', 'ocean', 'sky']
        
        for i in range(200):  # åˆ›å»º200å¼ æ¨¡æ‹Ÿå›¾åƒ
            # åˆ›å»ºéšæœºé¢œè‰²çš„å›¾åƒ
            image = PIL.Image.new('RGB', (224, 224), 
                                 color=(np.random.randint(0, 256), 
                                       np.random.randint(0, 256), 
                                       np.random.randint(0, 256)))
            images.append(image)
            labels.append(i % len(class_names))  # å¾ªç¯åˆ†é…æ ‡ç­¾
        
        return images, labels, class_names
    
    def preprocess_images(self, images, labels, test_size=0.2):
        """
        é¢„å¤„ç†å›¾åƒæ•°æ®
        Args:
            images: å›¾åƒåˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
        Returns:
            å¤„ç†åçš„æ•°æ®é›†
        """
        # åˆ’åˆ†æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            images, labels, test_size=test_size, random_state=42, stratify=labels
        )
        
        print(f"âœ… è®­ç»ƒé›†: {len(X_train)}å¼ å›¾åƒ")
        print(f"âœ… æµ‹è¯•é›†: {len(X_test)}å¼ å›¾åƒ")
        
        return X_train, X_test, y_train, y_test

class ViTTrainer:
    """
    ViTæ¨¡å‹è®­ç»ƒå™¨
    è´Ÿè´£æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
    """
    def __init__(self, model, processor, device):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        Args:
            model: ViTæ¨¡å‹
            processor: å›¾åƒå¤„ç†å™¨
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.processor = processor
        self.device = device
        
    def train_model(self, train_dataset, val_dataset, num_epochs=3, batch_size=8, learning_rate=5e-5):
        """
        è®­ç»ƒæ¨¡å‹
        Args:
            train_dataset: è®­ç»ƒæ•°æ®é›†
            val_dataset: éªŒè¯æ•°æ®é›†
            num_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
        """
        print("ğŸš€ å¼€å§‹è®­ç»ƒViTæ¨¡å‹...")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        train_accuracies = []
        val_accuracies = []
        
        for epoch in range(num_epochs):
            print(f"\nğŸ“š Epoch {epoch+1}/{num_epochs}")
            
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            for batch_idx, batch in enumerate(train_loader):
                optimizer.zero_grad()
                
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                pixel_values = batch['pixel_values'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                if HF_AVAILABLE:
                    outputs = self.model(pixel_values=pixel_values, labels=labels)
                    loss = outputs.loss
                    logits = outputs.logits
                else:
                    # æ¨¡æ‹Ÿè®­ç»ƒ
                    loss = torch.tensor(0.5, requires_grad=True)
                    logits = torch.randn(labels.size(0), self.model.config.num_labels)
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(logits, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                if batch_idx % 10 == 0:
                    print(f"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}")
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    pixel_values = batch['pixel_values'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    if HF_AVAILABLE:
                        outputs = self.model(pixel_values=pixel_values, labels=labels)
                        loss = outputs.loss
                        logits = outputs.logits
                    else:
                        loss = torch.tensor(0.3)
                        logits = torch.randn(labels.size(0), self.model.config.num_labels)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(logits, 1)
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
            
            # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            train_acc = 100. * train_correct / train_total
            val_acc = 100. * val_correct / val_total
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            train_accuracies.append(train_acc)
            val_accuracies.append(val_acc)
            
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
            print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        
        return train_losses, val_losses, train_accuracies, val_accuracies

class ImageClassificationEvaluator:
    """
    å›¾åƒåˆ†ç±»è¯„ä¼°å™¨
    è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
    """
    def __init__(self, class_names):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        Args:
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
        """
        self.class_names = class_names
    
    def evaluate_model(self, model, test_loader, device):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            device: è®¡ç®—è®¾å¤‡
        Returns:
            è¯„ä¼°ç»“æœ
        """
        model.eval()
        all_predictions = []
        all_labels = []
        all_probabilities = []
        
        with torch.no_grad():
            for batch in test_loader:
                pixel_values = batch['pixel_values'].to(device)
                labels = batch['labels'].to(device)
                
                if HF_AVAILABLE:
                    outputs = model(pixel_values=pixel_values)
                    logits = outputs.logits
                else:
                    logits = torch.randn(labels.size(0), model.config.num_labels)
                
                probabilities = torch.softmax(logits, dim=-1)
                _, predicted = torch.max(logits, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_predictions)
        report = classification_report(all_labels, all_predictions, 
                                    target_names=self.class_names)
        
        return accuracy, report, all_predictions, all_labels, all_probabilities
    
    def calculate_class_metrics(self, y_true, y_pred, y_prob):
        """
        è®¡ç®—å„ç±»åˆ«æŒ‡æ ‡
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            y_prob: é¢„æµ‹æ¦‚ç‡
        Returns:
            ç±»åˆ«æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        for i, class_name in enumerate(self.class_names):
            # è®¡ç®—è¯¥ç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
            tp = np.sum((y_true == i) & (y_pred == i))
            fp = np.sum((y_true != i) & (y_pred == i))
            fn = np.sum((y_true == i) & (y_pred != i))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            metrics[class_name] = {
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'support': np.sum(y_true == i)
            }
        
        return metrics

def visualize_training_history(train_losses, val_losses, train_accuracies, val_accuracies):
    """
    å¯è§†åŒ–è®­ç»ƒå†å²
    Args:
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        train_accuracies: è®­ç»ƒå‡†ç¡®ç‡åˆ—è¡¨
        val_accuracies: éªŒè¯å‡†ç¡®ç‡åˆ—è¡¨
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    epochs = range(1, len(train_losses) + 1)
    
    # æŸå¤±æ›²çº¿
    ax1.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', marker='o')
    ax1.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±', marker='s')
    ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('æŸå¤±')
    ax1.legend()
    ax1.grid(True)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(epochs, train_accuracies, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', marker='o')
    ax2.plot(epochs, val_accuracies, 'r-', label='éªŒè¯å‡†ç¡®ç‡', marker='s')
    ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax2.legend()
    ax2.grid(True)
    
    # æŸå¤±å¯¹æ¯”
    x = np.arange(len(epochs))
    width = 0.35
    ax3.bar(x - width/2, train_losses, width, label='è®­ç»ƒæŸå¤±', alpha=0.8)
    ax3.bar(x + width/2, val_losses, width, label='éªŒè¯æŸå¤±', alpha=0.8)
    ax3.set_title('æŸå¤±å¯¹æ¯”')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('æŸå¤±')
    ax3.set_xticks(x)
    ax3.set_xticklabels(epochs)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    ax4.bar(x - width/2, train_accuracies, width, label='è®­ç»ƒå‡†ç¡®ç‡', alpha=0.8)
    ax4.bar(x + width/2, val_accuracies, width, label='éªŒè¯å‡†ç¡®ç‡', alpha=0.8)
    ax4.set_title('å‡†ç¡®ç‡å¯¹æ¯”')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax4.set_xticks(x)
    ax4.set_xticklabels(epochs)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('nlp_base/vit_training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def visualize_classification_results(y_true, y_pred, class_names, y_prob=None):
    """
    å¯è§†åŒ–åˆ†ç±»ç»“æœ
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        class_names: ç±»åˆ«åç§°
        y_prob: é¢„æµ‹æ¦‚ç‡
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names, ax=ax1)
    ax1.set_title('æ··æ·†çŸ©é˜µ')
    ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾')
    ax1.set_ylabel('çœŸå®æ ‡ç­¾')
    
    # ç±»åˆ«åˆ†å¸ƒ
    true_counts = [np.sum(y_true == i) for i in range(len(class_names))]
    pred_counts = [np.sum(y_pred == i) for i in range(len(class_names))]
    
    x = np.arange(len(class_names))
    width = 0.35
    ax2.bar(x - width/2, true_counts, width, label='çœŸå®åˆ†å¸ƒ', alpha=0.8)
    ax2.bar(x + width/2, pred_counts, width, label='é¢„æµ‹åˆ†å¸ƒ', alpha=0.8)
    ax2.set_title('ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”')
    ax2.set_xlabel('ç±»åˆ«')
    ax2.set_ylabel('æ ·æœ¬æ•°é‡')
    ax2.set_xticks(x)
    ax2.set_xticklabels(class_names, rotation=45)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡åˆ†æ
    class_accuracies = []
    for i in range(len(class_names)):
        class_mask = y_true == i
        if np.sum(class_mask) > 0:
            accuracy = np.sum((y_pred[class_mask] == i)) / np.sum(class_mask)
            class_accuracies.append(accuracy)
        else:
            class_accuracies.append(0)
    
    ax3.bar(class_names, class_accuracies, alpha=0.7, color='skyblue')
    ax3.set_title('å„ç±»åˆ«å‡†ç¡®ç‡')
    ax3.set_xlabel('ç±»åˆ«')
    ax3.set_ylabel('å‡†ç¡®ç‡')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(True, alpha=0.3)
    
    # é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
    if y_prob is not None:
        max_probs = np.max(y_prob, axis=1)
        ax4.hist(max_probs, bins=20, alpha=0.7, color='lightgreen')
        ax4.set_title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        ax4.set_xlabel('æœ€å¤§é¢„æµ‹æ¦‚ç‡')
        ax4.set_ylabel('é¢‘æ¬¡')
        ax4.grid(True, alpha=0.3)
    else:
        ax4.text(0.5, 0.5, 'ç½®ä¿¡åº¦æ•°æ®ä¸å¯ç”¨', ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.savefig('nlp_base/vit_classification_results.png', dpi=300, bbox_inches='tight')
    plt.show()

def visualize_sample_predictions(images, predictions, class_names, num_samples=8):
    """
    å¯è§†åŒ–æ ·æœ¬é¢„æµ‹ç»“æœ
    Args:
        images: å›¾åƒåˆ—è¡¨
        predictions: é¢„æµ‹ç»“æœ
        class_names: ç±»åˆ«åç§°
        num_samples: æ˜¾ç¤ºæ ·æœ¬æ•°é‡
    """
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.flatten()
    
    for i in range(min(num_samples, len(images))):
        ax = axes[i]
        
        # æ˜¾ç¤ºå›¾åƒ
        if hasattr(images[i], 'convert'):
            ax.imshow(images[i])
        else:
            # å¦‚æœæ˜¯numpyæ•°ç»„
            ax.imshow(images[i])
        
        # è®¾ç½®æ ‡é¢˜
        pred_class = class_names[predictions[i]]
        ax.set_title(f'é¢„æµ‹: {pred_class}', fontsize=12)
        ax.axis('off')
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(num_samples, len(axes)):
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.savefig('nlp_base/vit_sample_predictions.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """
    ä¸»å‡½æ•°
    æ‰§è¡Œå®Œæ•´çš„å›¾åƒåˆ†ç±»æµç¨‹
    """
    print("=" * 80)
    print("ğŸ–¼ï¸ åŸºäºVision Transformer (ViT) çš„å›¾åƒåˆ†ç±»ç¨‹åº")
    print("ğŸ“š ä½¿ç”¨HuggingFace Transformersåº“å®ç°å›¾åƒåˆ†ç±»ä»»åŠ¡")
    print("=" * 80)
    
    # è®¾ç½®å‚æ•°
    BATCH_SIZE = 8
    NUM_EPOCHS = 3
    LEARNING_RATE = 5e-5
    NUM_LABELS = 10
    
    # é€‰æ‹©è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½å’Œå¤„ç†æ•°æ®
    print("\nğŸ“Š 1. åŠ è½½å’Œå¤„ç†å›¾åƒæ•°æ®...")
    processor = ImageDataProcessor()
    
    # åŠ è½½æ•°æ®
    images, labels, class_names = processor.load_cifar10_data()
    
    # é¢„å¤„ç†æ•°æ®
    X_train, X_test, y_train, y_test = processor.preprocess_images(images, labels)
    
    print(f"âœ… è®­ç»ƒæ•°æ®: {len(X_train)}å¼ å›¾åƒ")
    print(f"âœ… æµ‹è¯•æ•°æ®: {len(X_test)}å¼ å›¾åƒ")
    print(f"âœ… ç±»åˆ«: {class_names}")
    
    # 2. åˆå§‹åŒ–æ¨¡å‹å’Œå¤„ç†å™¨
    print("\nğŸ§  2. åˆå§‹åŒ–ViTæ¨¡å‹å’Œå¤„ç†å™¨...")
    classifier = ViTImageClassifier(
        model_name='google/vit-base-patch16-224',
        num_labels=NUM_LABELS,
        device=device
    )
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = ImageDataset(X_train, y_train, classifier.processor)
    test_dataset = ImageDataset(X_test, y_test, classifier.processor)
    
    print(f"âœ… è®­ç»ƒæ•°æ®é›†: {len(train_dataset)}ä¸ªæ ·æœ¬")
    print(f"âœ… æµ‹è¯•æ•°æ®é›†: {len(test_dataset)}ä¸ªæ ·æœ¬")
    
    # 3. è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ 3. å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    trainer = ViTTrainer(classifier.model, classifier.processor, device)
    
    start_time = time.time()
    train_losses, val_losses, train_accuracies, val_accuracies = trainer.train_model(
        train_dataset, test_dataset, 
        num_epochs=NUM_EPOCHS, 
        batch_size=BATCH_SIZE, 
        learning_rate=LEARNING_RATE
    )
    training_time = time.time() - start_time
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶: {training_time:.2f}ç§’")
    
    # 4. è¯„ä¼°æ¨¡å‹
    print("\nğŸ“ˆ 4. è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    evaluator = ImageClassificationEvaluator(class_names)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    # è¯„ä¼°æ¨¡å‹
    accuracy, report, predictions, true_labels, probabilities = evaluator.evaluate_model(
        classifier.model, test_loader, device
    )
    
    print(f"ğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(report)
    
    # 5. ç”Ÿæˆé¢„æµ‹ç»“æœ
    print("\nâœ¨ 5. ç”Ÿæˆé¢„æµ‹ç»“æœ...")
    
    # é€‰æ‹©ä¸€äº›æµ‹è¯•æ ·æœ¬è¿›è¡Œé¢„æµ‹
    sample_images = X_test[:8]
    sample_predictions, sample_probabilities = classifier.predict(sample_images)
    
    print("ğŸ“ æ ·æœ¬é¢„æµ‹ç»“æœ:")
    for i, (img, pred) in enumerate(zip(sample_images, sample_predictions)):
        pred_class = class_names[pred]
        confidence = np.max(sample_probabilities[i])
        print(f"  æ ·æœ¬ {i+1}: {pred_class} (ç½®ä¿¡åº¦: {confidence:.3f})")
    
    # 6. è®¡ç®—è¯¦ç»†æŒ‡æ ‡
    print("\nğŸ“Š 6. è®¡ç®—è¯¦ç»†æŒ‡æ ‡...")
    class_metrics = evaluator.calculate_class_metrics(true_labels, predictions, probabilities)
    
    print("ğŸ“ˆ å„ç±»åˆ«æŒ‡æ ‡:")
    for class_name, metrics in class_metrics.items():
        print(f"  {class_name}:")
        print(f"    ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
        print(f"    å¬å›ç‡: {metrics['recall']:.4f}")
        print(f"    F1åˆ†æ•°: {metrics['f1_score']:.4f}")
        print(f"    æ”¯æŒæ•°: {metrics['support']}")
    
    # 7. å¯è§†åŒ–ç»“æœ
    print("\nğŸ“Š 7. ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    visualize_training_history(train_losses, val_losses, train_accuracies, val_accuracies)
    visualize_classification_results(true_labels, predictions, class_names, probabilities)
    visualize_sample_predictions(sample_images, sample_predictions, class_names)
    
    # 8. ä¿å­˜æ¨¡å‹å’Œç»“æœ
    print("\nğŸ’¾ 8. ä¿å­˜æ¨¡å‹å’Œç»“æœ...")
    
    # ä¿å­˜æ¨¡å‹
    if HF_AVAILABLE:
        classifier.model.save_pretrained('nlp_base/vit_image_classifier_model')
        classifier.processor.save_pretrained('nlp_base/vit_image_classifier_model')
        print("âœ… æ¨¡å‹å·²ä¿å­˜")
    
    # ä¿å­˜ç»“æœ
    results = {
        'training_time': training_time,
        'test_accuracy': accuracy,
        'class_metrics': class_metrics,
        'sample_predictions': [
            {
                'image_index': i,
                'predicted_class': class_names[pred],
                'confidence': float(np.max(prob))
            }
            for i, (pred, prob) in enumerate(zip(sample_predictions, sample_probabilities))
        ],
        'model_config': {
            'model_name': classifier.model_name,
            'num_labels': NUM_LABELS,
            'batch_size': BATCH_SIZE,
            'num_epochs': NUM_EPOCHS,
            'learning_rate': LEARNING_RATE
        }
    }
    
    with open('nlp_base/vit_classification_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("âœ… ç»“æœå·²ä¿å­˜åˆ° vit_classification_results.json")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ å›¾åƒåˆ†ç±»ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - vit_training_history.png (è®­ç»ƒå†å²)")
    print("  - vit_classification_results.png (åˆ†ç±»ç»“æœ)")
    print("  - vit_sample_predictions.png (æ ·æœ¬é¢„æµ‹)")
    print("  - vit_classification_results.json (è¯¦ç»†ç»“æœ)")
    if HF_AVAILABLE:
        print("  - vit_image_classifier_model/ (ä¿å­˜çš„æ¨¡å‹)")
    print("=" * 80)

if __name__ == "__main__":
    main()
