"""
åŸºäºPyTorchå’ŒHuggingFace Transformersçš„æ–‡æœ¬ç”Ÿæˆç¨‹åº
ä½¿ç”¨WikiText-2æ•°æ®é›†å®ç°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
æ”¯æŒGPT-2æ¨¡å‹è®­ç»ƒã€å¾®è°ƒå’Œæ–‡æœ¬ç”Ÿæˆ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import re
import os
import time
import json
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥HuggingFaceåº“
try:
    from transformers import (
        GPT2LMHeadModel, GPT2Tokenizer, GPT2Config,
        TrainingArguments, Trainer, DataCollatorForLanguageModeling,
        get_linear_schedule_with_warmup
    )
    from datasets import load_dataset
    HF_AVAILABLE = True
    print("âœ… HuggingFace Transformersåº“å¯ç”¨")
except ImportError:
    HF_AVAILABLE = False
    print("âŒ HuggingFace Transformersåº“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

class WikiTextDataset(Dataset):
    """
    WikiText-2æ•°æ®é›†å¤„ç†ç±»
    å¤„ç†æ–‡æœ¬æ•°æ®ï¼Œæ”¯æŒè®­ç»ƒå’ŒéªŒè¯
    """
    def __init__(self, texts, tokenizer, max_length=512):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.texts)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        Args:
            idx: æ ·æœ¬ç´¢å¼•
        Returns:
            ç¼–ç åçš„æ–‡æœ¬å¼ é‡
        """
        text = self.texts[idx]
        
        # ä½¿ç”¨åˆ†è¯å™¨ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()  # å¯¹äºè¯­è¨€æ¨¡å‹ï¼Œæ ‡ç­¾å°±æ˜¯è¾“å…¥
        }

class TextGenerator:
    """
    æ–‡æœ¬ç”Ÿæˆå™¨ç±»
    åŸºäºGPT-2æ¨¡å‹å®ç°æ–‡æœ¬ç”ŸæˆåŠŸèƒ½
    """
    def __init__(self, model_name='gpt2', device=None):
        """
        åˆå§‹åŒ–æ–‡æœ¬ç”Ÿæˆå™¨
        Args:
            model_name: æ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_name = model_name
        
        if HF_AVAILABLE:
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
            self.model = GPT2LMHeadModel.from_pretrained(model_name)
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model.to(self.device)
            print(f"âœ… åŠ è½½æ¨¡å‹: {model_name}")
        else:
            # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
            self.tokenizer = self._create_mock_tokenizer()
            self.model = self._create_mock_model()
            print("âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ï¼ˆHuggingFaceåº“ä¸å¯ç”¨ï¼‰")
    
    def _create_mock_tokenizer(self):
        """åˆ›å»ºæ¨¡æ‹Ÿåˆ†è¯å™¨"""
        class MockTokenizer:
            def __init__(self):
                self.vocab = {
                    '<pad>': 0, '<unk>': 1, '<s>': 2, '</s>': 3,
                    'the': 4, 'a': 5, 'an': 6, 'and': 7, 'or': 8, 'but': 9,
                    'is': 10, 'are': 11, 'was': 12, 'were': 13,
                    'i': 14, 'you': 15, 'he': 16, 'she': 17, 'it': 18,
                    'we': 19, 'they': 20, 'this': 21, 'that': 22,
                    'in': 23, 'on': 24, 'at': 25, 'to': 26, 'for': 27,
                    'of': 28, 'with': 29, 'by': 30, 'from': 31
                }
                self.reverse_vocab = {v: k for k, v in self.vocab.items()}
                self.pad_token = '<pad>'
                self.eos_token = '</s>'
                self.bos_token = '<s>'
            
            def encode(self, text, **kwargs):
                """ç¼–ç æ–‡æœ¬"""
                words = text.lower().split()
                ids = [self.vocab.get(word, self.vocab['<unk>']) for word in words]
                return ids
            
            def decode(self, ids, **kwargs):
                """è§£ç æ–‡æœ¬"""
                words = [self.reverse_vocab.get(id, '<unk>') for id in ids]
                return ' '.join(words)
            
            def __call__(self, text, **kwargs):
                """åˆ†è¯å™¨è°ƒç”¨"""
                return {'input_ids': torch.tensor(self.encode(text))}
        
        return MockTokenizer()
    
    def _create_mock_model(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹"""
        class MockModel:
            def __init__(self):
                self.config = type('Config', (), {'vocab_size': 32})()
            
            def generate(self, input_ids, max_length=50, **kwargs):
                """æ¨¡æ‹Ÿæ–‡æœ¬ç”Ÿæˆ"""
                batch_size, seq_len = input_ids.shape
                # ç®€å•çš„éšæœºç”Ÿæˆ
                generated = input_ids.clone()
                for i in range(seq_len, max_length):
                    next_token = torch.randint(1, 32, (batch_size, 1))
                    generated = torch.cat([generated, next_token], dim=1)
                return generated
            
            def to(self, device):
                return self
            
            def train(self):
                pass
            
            def eval(self):
                pass
        
        return MockModel()
    
    def generate_text(self, prompt, max_length=100, temperature=0.8, top_p=0.9, num_return_sequences=1):
        """
        ç”Ÿæˆæ–‡æœ¬
        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            top_p: æ ¸é‡‡æ ·å‚æ•°
            num_return_sequences: ç”Ÿæˆåºåˆ—æ•°é‡
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        self.model.eval()
        
        # ç¼–ç è¾“å…¥æç¤º
        if HF_AVAILABLE:
            input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
        else:
            input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)
        
        with torch.no_grad():
            if HF_AVAILABLE:
                # ä½¿ç”¨HuggingFaceæ¨¡å‹ç”Ÿæˆ
                outputs = self.model.generate(
                    input_ids,
                    max_length=max_length,
                    temperature=temperature,
                    top_p=top_p,
                    num_return_sequences=num_return_sequences,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    do_sample=True
                )
            else:
                # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ç”Ÿæˆ
                outputs = self.model.generate(
                    input_ids,
                    max_length=max_length
                )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_texts = []
        for output in outputs:
            if HF_AVAILABLE:
                text = self.tokenizer.decode(output, skip_special_tokens=True)
            else:
                text = self.tokenizer.decode(output.tolist())
            generated_texts.append(text)
        
        return generated_texts

class WikiTextProcessor:
    """
    WikiText-2æ•°æ®å¤„ç†å™¨
    å¤„ç†åŸå§‹æ–‡æœ¬æ•°æ®ï¼Œå‡†å¤‡è®­ç»ƒæ•°æ®
    """
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨"""
        self.texts = []
        self.processed_texts = []
    
    def load_wikitext_data(self, split='train'):
        """
        åŠ è½½WikiText-2æ•°æ®
        Args:
            split: æ•°æ®é›†åˆ†å‰²ï¼ˆtrain/validation/testï¼‰
        Returns:
            æ–‡æœ¬åˆ—è¡¨
        """
        if HF_AVAILABLE:
            try:
                # ä½¿ç”¨HuggingFace datasetsåº“åŠ è½½
                dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split=split)
                texts = [item['text'] for item in dataset if item['text'].strip()]
                print(f"âœ… åŠ è½½WikiText-2 {split}æ•°æ®: {len(texts)}æ¡")
                return texts
            except Exception as e:
                print(f"âŒ åŠ è½½WikiText-2å¤±è´¥: {e}")
                return self._create_mock_data()
        else:
            return self._create_mock_data()
    
    def _create_mock_data(self):
        """åˆ›å»ºæ¨¡æ‹ŸWikiTextæ•°æ®"""
        print("âš ï¸ åˆ›å»ºæ¨¡æ‹ŸWikiTextæ•°æ®")
        
        mock_texts = [
            "The quick brown fox jumps over the lazy dog.",
            "Machine learning is a subset of artificial intelligence.",
            "Natural language processing deals with human language understanding.",
            "Deep learning models can process large amounts of text data.",
            "Transformers have revolutionized the field of NLP.",
            "Attention mechanisms allow models to focus on relevant information.",
            "Language models can generate coherent and fluent text.",
            "Text generation is an important task in natural language processing.",
            "Neural networks can learn complex patterns from data.",
            "Pre-trained models have improved performance on many NLP tasks.",
            "The development of large language models has advanced AI capabilities.",
            "Text understanding and generation are key challenges in AI.",
            "Modern NLP systems use transformer architectures extensively.",
            "Language models can be fine-tuned for specific tasks.",
            "The quality of generated text depends on training data and model size."
        ]
        
        # æ‰©å±•æ•°æ®
        extended_texts = []
        for _ in range(100):  # ç”Ÿæˆæ›´å¤šæ ·æœ¬
            for text in mock_texts:
                extended_texts.append(text)
        
        return extended_texts
    
    def preprocess_text(self, texts, min_length=10, max_length=512):
        """
        é¢„å¤„ç†æ–‡æœ¬æ•°æ®
        Args:
            texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
            min_length: æœ€å°æ–‡æœ¬é•¿åº¦
            max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
        Returns:
            å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨
        """
        processed = []
        
        for text in texts:
            # æ¸…ç†æ–‡æœ¬
            text = text.strip()
            
            # è¿‡æ»¤ç©ºæ–‡æœ¬å’Œè¿‡çŸ­æ–‡æœ¬
            if len(text) < min_length:
                continue
            
            # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
            if len(text) > max_length:
                text = text[:max_length]
            
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            text = re.sub(r'[^\w\s.,!?;:]', '', text)
            
            processed.append(text)
        
        print(f"âœ… é¢„å¤„ç†å®Œæˆ: {len(processed)}æ¡æœ‰æ•ˆæ–‡æœ¬")
        return processed

class TextGenerationTrainer:
    """
    æ–‡æœ¬ç”Ÿæˆè®­ç»ƒå™¨
    è´Ÿè´£æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
    """
    def __init__(self, model, tokenizer, device):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        Args:
            model: è¯­è¨€æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
    def train_model(self, train_dataset, val_dataset, num_epochs=3, batch_size=4, learning_rate=5e-5):
        """
        è®­ç»ƒæ¨¡å‹
        Args:
            train_dataset: è®­ç»ƒæ•°æ®é›†
            val_dataset: éªŒè¯æ•°æ®é›†
            num_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
        """
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss(ignore_index=-100)
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        
        for epoch in range(num_epochs):
            print(f"\nğŸ“š Epoch {epoch+1}/{num_epochs}")
            
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            
            for batch_idx, batch in enumerate(train_loader):
                optimizer.zero_grad()
                
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                if HF_AVAILABLE:
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss
                else:
                    # æ¨¡æ‹Ÿè®­ç»ƒ
                    loss = torch.tensor(0.5, requires_grad=True)
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                
                if batch_idx % 10 == 0:
                    print(f"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}")
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    if HF_AVAILABLE:
                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                        loss = outputs.loss
                    else:
                        loss = torch.tensor(0.3)
                    
                    val_loss += loss.item()
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        
        return train_losses, val_losses

class TextGenerationEvaluator:
    """
    æ–‡æœ¬ç”Ÿæˆè¯„ä¼°å™¨
    è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
    """
    def __init__(self, tokenizer):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        Args:
            tokenizer: åˆ†è¯å™¨
        """
        self.tokenizer = tokenizer
    
    def calculate_perplexity(self, model, test_dataset):
        """
        è®¡ç®—å›°æƒ‘åº¦
        Args:
            model: è¯­è¨€æ¨¡å‹
            test_dataset: æµ‹è¯•æ•°æ®é›†
        Returns:
            å›°æƒ‘åº¦å€¼
        """
        model.eval()
        total_loss = 0
        total_tokens = 0
        
        with torch.no_grad():
            for item in test_dataset:
                input_ids = item['input_ids'].unsqueeze(0)
                labels = item['labels'].unsqueeze(0)
                
                if HF_AVAILABLE:
                    outputs = model(input_ids=input_ids, labels=labels)
                    loss = outputs.loss
                else:
                    loss = torch.tensor(0.4)  # æ¨¡æ‹ŸæŸå¤±
                
                total_loss += loss.item() * input_ids.size(1)
                total_tokens += input_ids.size(1)
        
        avg_loss = total_loss / total_tokens
        perplexity = np.exp(avg_loss)
        
        return perplexity
    
    def evaluate_generation_quality(self, generated_texts, reference_texts):
        """
        è¯„ä¼°ç”Ÿæˆè´¨é‡
        Args:
            generated_texts: ç”Ÿæˆçš„æ–‡æœ¬
            reference_texts: å‚è€ƒæ–‡æœ¬
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
        avg_length = np.mean([len(text.split()) for text in generated_texts])
        unique_words = len(set(word for text in generated_texts for word in text.split()))
        
        # è®¡ç®—é‡å¤ç‡
        repetition_rate = self._calculate_repetition_rate(generated_texts)
        
        # è®¡ç®—æµç•…åº¦ï¼ˆç®€å•çš„n-gramé‡å ï¼‰
        fluency_score = self._calculate_fluency(generated_texts)
        
        return {
            'average_length': avg_length,
            'unique_words': unique_words,
            'repetition_rate': repetition_rate,
            'fluency_score': fluency_score
        }
    
    def _calculate_repetition_rate(self, texts):
        """è®¡ç®—é‡å¤ç‡"""
        total_repetitions = 0
        total_words = 0
        
        for text in texts:
            words = text.split()
            total_words += len(words)
            
            # è®¡ç®—é‡å¤çš„n-gram
            for n in [2, 3, 4]:
                ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]
                unique_ngrams = set(ngrams)
                repetitions = len(ngrams) - len(unique_ngrams)
                total_repetitions += repetitions
        
        return total_repetitions / max(total_words, 1)
    
    def _calculate_fluency(self, texts):
        """è®¡ç®—æµç•…åº¦"""
        # ç®€å•çš„æµç•…åº¦è®¡ç®—ï¼šåŸºäºè¯æ±‡å¤šæ ·æ€§
        all_words = []
        for text in texts:
            all_words.extend(text.split())
        
        unique_words = set(all_words)
        total_words = len(all_words)
        
        return len(unique_words) / max(total_words, 1)

def visualize_training_history(train_losses, val_losses):
    """
    å¯è§†åŒ–è®­ç»ƒå†å²
    Args:
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
    """
    plt.figure(figsize=(12, 5))
    
    # æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue', marker='o')
    plt.plot(val_losses, label='éªŒè¯æŸå¤±', color='red', marker='s')
    plt.title('è®­ç»ƒå†å²')
    plt.xlabel('Epoch')
    plt.ylabel('æŸå¤±')
    plt.legend()
    plt.grid(True)
    
    # æŸå¤±å¯¹æ¯”
    plt.subplot(1, 2, 2)
    epochs = range(1, len(train_losses) + 1)
    x = np.arange(len(epochs))
    width = 0.35
    
    plt.bar(x - width/2, train_losses, width, label='è®­ç»ƒæŸå¤±', alpha=0.8)
    plt.bar(x + width/2, val_losses, width, label='éªŒè¯æŸå¤±', alpha=0.8)
    plt.title('æŸå¤±å¯¹æ¯”')
    plt.xlabel('Epoch')
    plt.ylabel('æŸå¤±')
    plt.xticks(x, epochs)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('nlp_base/text_generation_training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def visualize_generation_results(generated_texts, metrics):
    """
    å¯è§†åŒ–ç”Ÿæˆç»“æœ
    Args:
        generated_texts: ç”Ÿæˆçš„æ–‡æœ¬
        metrics: è¯„ä¼°æŒ‡æ ‡
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
    lengths = [len(text.split()) for text in generated_texts]
    axes[0, 0].hist(lengths, bins=20, alpha=0.7, color='skyblue')
    axes[0, 0].set_title('ç”Ÿæˆæ–‡æœ¬é•¿åº¦åˆ†å¸ƒ')
    axes[0, 0].set_xlabel('æ–‡æœ¬é•¿åº¦ï¼ˆè¯æ•°ï¼‰')
    axes[0, 0].set_ylabel('é¢‘æ¬¡')
    axes[0, 0].grid(True, alpha=0.3)
    
    # è¯„ä¼°æŒ‡æ ‡é›·è¾¾å›¾
    categories = ['å¹³å‡é•¿åº¦', 'ç‹¬ç‰¹è¯æ±‡', 'æµç•…åº¦', 'é‡å¤ç‡']
    values = [
        metrics['average_length'] / 50,  # å½’ä¸€åŒ–
        metrics['unique_words'] / 1000,  # å½’ä¸€åŒ–
        metrics['fluency_score'],
        1 - metrics['repetition_rate']  # åè½¬é‡å¤ç‡
    ]
    
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    values += values[:1]  # é—­åˆå›¾å½¢
    angles += angles[:1]
    
    axes[0, 1].plot(angles, values, 'o-', linewidth=2, color='red')
    axes[0, 1].fill(angles, values, alpha=0.25, color='red')
    axes[0, 1].set_xticks(angles[:-1])
    axes[0, 1].set_xticklabels(categories)
    axes[0, 1].set_title('ç”Ÿæˆè´¨é‡è¯„ä¼°')
    axes[0, 1].grid(True)
    
    # è¯æ±‡é¢‘ç‡
    all_words = []
    for text in generated_texts:
        all_words.extend(text.split())
    
    word_counts = Counter(all_words)
    top_words = dict(word_counts.most_common(10))
    
    axes[1, 0].bar(range(len(top_words)), list(top_words.values()))
    axes[1, 0].set_title('é«˜é¢‘è¯æ±‡')
    axes[1, 0].set_xlabel('è¯æ±‡')
    axes[1, 0].set_ylabel('é¢‘æ¬¡')
    axes[1, 0].set_xticks(range(len(top_words)))
    axes[1, 0].set_xticklabels(list(top_words.keys()), rotation=45)
    axes[1, 0].grid(True, alpha=0.3)
    
    # ç”Ÿæˆç¤ºä¾‹
    axes[1, 1].text(0.1, 0.9, 'ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹:', fontsize=12, fontweight='bold', transform=axes[1, 1].transAxes)
    sample_text = generated_texts[0][:200] + '...' if len(generated_texts[0]) > 200 else generated_texts[0]
    axes[1, 1].text(0.1, 0.7, sample_text, fontsize=10, transform=axes[1, 1].transAxes, 
                    verticalalignment='top', wrap=True)
    axes[1, 1].set_title('ç”Ÿæˆç¤ºä¾‹')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig('nlp_base/text_generation_results.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """
    ä¸»å‡½æ•°
    æ‰§è¡Œå®Œæ•´çš„æ–‡æœ¬ç”Ÿæˆæµç¨‹
    """
    print("=" * 80)
    print("ğŸ¤– åŸºäºPyTorchå’ŒHuggingFace Transformersçš„æ–‡æœ¬ç”Ÿæˆç¨‹åº")
    print("ğŸ“š ä½¿ç”¨WikiText-2æ•°æ®é›†å®ç°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡")
    print("=" * 80)
    
    # è®¾ç½®å‚æ•°
    BATCH_SIZE = 4
    MAX_LENGTH = 256
    NUM_EPOCHS = 3
    LEARNING_RATE = 5e-5
    
    # é€‰æ‹©è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½å’Œå¤„ç†æ•°æ®
    print("\nğŸ“Š 1. åŠ è½½å’Œå¤„ç†WikiText-2æ•°æ®...")
    processor = WikiTextProcessor()
    
    # åŠ è½½æ•°æ®
    train_texts = processor.load_wikitext_data('train')
    val_texts = processor.load_wikitext_data('validation')
    
    # é¢„å¤„ç†æ•°æ®
    train_texts = processor.preprocess_text(train_texts, min_length=20, max_length=MAX_LENGTH)
    val_texts = processor.preprocess_text(val_texts, min_length=20, max_length=MAX_LENGTH)
    
    print(f"âœ… è®­ç»ƒæ•°æ®: {len(train_texts)}æ¡")
    print(f"âœ… éªŒè¯æ•°æ®: {len(val_texts)}æ¡")
    
    # 2. åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    print("\nğŸ§  2. åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨...")
    generator = TextGenerator(device=device)
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = WikiTextDataset(train_texts, generator.tokenizer, MAX_LENGTH)
    val_dataset = WikiTextDataset(val_texts, generator.tokenizer, MAX_LENGTH)
    
    print(f"âœ… è®­ç»ƒæ•°æ®é›†: {len(train_dataset)}ä¸ªæ ·æœ¬")
    print(f"âœ… éªŒè¯æ•°æ®é›†: {len(val_dataset)}ä¸ªæ ·æœ¬")
    
    # 3. è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ 3. å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    trainer = TextGenerationTrainer(generator.model, generator.tokenizer, device)
    
    start_time = time.time()
    train_losses, val_losses = trainer.train_model(
        train_dataset, val_dataset, 
        num_epochs=NUM_EPOCHS, 
        batch_size=BATCH_SIZE, 
        learning_rate=LEARNING_RATE
    )
    training_time = time.time() - start_time
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶: {training_time:.2f}ç§’")
    
    # 4. è¯„ä¼°æ¨¡å‹
    print("\nğŸ“ˆ 4. è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    evaluator = TextGenerationEvaluator(generator.tokenizer)
    
    # è®¡ç®—å›°æƒ‘åº¦
    perplexity = evaluator.calculate_perplexity(generator.model, val_dataset)
    print(f"ğŸ“Š å›°æƒ‘åº¦: {perplexity:.2f}")
    
    # 5. ç”Ÿæˆæ–‡æœ¬
    print("\nâœ¨ 5. ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹...")
    
    # æµ‹è¯•æç¤º
    test_prompts = [
        "The future of artificial intelligence",
        "Machine learning is",
        "Natural language processing",
        "Deep learning models",
        "Transformers have revolutionized"
    ]
    
    generated_texts = []
    for prompt in test_prompts:
        print(f"\nğŸ¯ æç¤º: '{prompt}'")
        generated = generator.generate_text(
            prompt, 
            max_length=100, 
            temperature=0.8, 
            num_return_sequences=1
        )
        print(f"ğŸ“ ç”Ÿæˆ: '{generated[0]}'")
        generated_texts.extend(generated)
    
    # 6. è¯„ä¼°ç”Ÿæˆè´¨é‡
    print("\nğŸ“Š 6. è¯„ä¼°ç”Ÿæˆè´¨é‡...")
    metrics = evaluator.evaluate_generation_quality(generated_texts, val_texts[:len(generated_texts)])
    
    print("ğŸ“ˆ ç”Ÿæˆè´¨é‡æŒ‡æ ‡:")
    for key, value in metrics.items():
        print(f"  {key}: {value:.4f}")
    
    # 7. å¯è§†åŒ–ç»“æœ
    print("\nğŸ“Š 7. ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    visualize_training_history(train_losses, val_losses)
    visualize_generation_results(generated_texts, metrics)
    
    # 8. ä¿å­˜æ¨¡å‹å’Œç»“æœ
    print("\nğŸ’¾ 8. ä¿å­˜æ¨¡å‹å’Œç»“æœ...")
    
    # ä¿å­˜æ¨¡å‹
    if HF_AVAILABLE:
        generator.model.save_pretrained('nlp_base/text_generation_model')
        generator.tokenizer.save_pretrained('nlp_base/text_generation_model')
        print("âœ… æ¨¡å‹å·²ä¿å­˜")
    
    # ä¿å­˜ç»“æœ
    results = {
        'training_time': training_time,
        'perplexity': perplexity,
        'metrics': metrics,
        'generated_texts': generated_texts[:5],  # ä¿å­˜å‰5ä¸ªç”Ÿæˆæ ·æœ¬
        'model_config': {
            'model_name': generator.model_name,
            'max_length': MAX_LENGTH,
            'batch_size': BATCH_SIZE,
            'num_epochs': NUM_EPOCHS,
            'learning_rate': LEARNING_RATE
        }
    }
    
    with open('nlp_base/text_generation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("âœ… ç»“æœå·²ä¿å­˜åˆ° text_generation_results.json")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æ–‡æœ¬ç”Ÿæˆç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - text_generation_training_history.png (è®­ç»ƒå†å²)")
    print("  - text_generation_results.png (ç”Ÿæˆç»“æœ)")
    print("  - text_generation_results.json (è¯¦ç»†ç»“æœ)")
    if HF_AVAILABLE:
        print("  - text_generation_model/ (ä¿å­˜çš„æ¨¡å‹)")
    print("=" * 80)

if __name__ == "__main__":
    main()
